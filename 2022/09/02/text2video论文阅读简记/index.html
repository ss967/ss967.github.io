<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>text2video论文阅读简记 | 星空Blog</title><meta name="keywords" content="深度学习,text2video,视频生成,跨模态"><meta name="author" content="ss967"><meta name="copyright" content="ss967"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Text2Video: Text-driven Talking-head Video Synthesis with Personalized Phoneme - Pose Dictionary 论文连接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2104.14631 GitHub代码地址：https:&#x2F;&#x2F;github.com&#x2F;sibozhang&#x2F;Text2Video     （只有测试代码，没有">
<meta property="og:type" content="article">
<meta property="og:title" content="text2video论文阅读简记">
<meta property="og:url" content="http://ss967.github.io/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/index.html">
<meta property="og:site_name" content="星空Blog">
<meta property="og:description" content="Text2Video: Text-driven Talking-head Video Synthesis with Personalized Phoneme - Pose Dictionary 论文连接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2104.14631 GitHub代码地址：https:&#x2F;&#x2F;github.com&#x2F;sibozhang&#x2F;Text2Video     （只有测试代码，没有">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://ss967.github.io/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/cover.png">
<meta property="article:published_time" content="2022-09-02T05:25:28.000Z">
<meta property="article:modified_time" content="2022-09-23T12:17:57.251Z">
<meta property="article:author" content="ss967">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="text2video">
<meta property="article:tag" content="视频生成">
<meta property="article:tag" content="跨模态">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://ss967.github.io/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/cover.png"><link rel="shortcut icon" href="/img/logo.ico"><link rel="canonical" href="http://ss967.github.io/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?506834e66449f901499a22592120422e";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#9370DB","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'text2video论文阅读简记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-09-23 20:17:57'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_2928498_ppl07sthm9q.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/rss2.xml" title="星空Blog" type="application/rss+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/top.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单列表</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> 书籍</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/cover.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">星空Blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单列表</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> 书籍</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">text2video论文阅读简记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-02T05:25:28.000Z" title="发表于 2022-09-02 13:25:28">2022-09-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-09-23T12:17:57.251Z" title="更新于 2022-09-23 20:17:57">2022-09-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>10分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="text2video论文阅读简记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Text2Video-Text-driven-Talking-head-Video-Synthesis-with-Personalized-Phoneme-Pose-Dictionary">Text2Video: Text-driven Talking-head Video Synthesis with Personalized Phoneme - Pose Dictionary</h2>
<p><strong>论文连接</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.14631">https://arxiv.org/abs/2104.14631</a></p>
<p><strong>GitHub代码地址</strong>：<a target="_blank" rel="noopener" href="https://github.com/sibozhang/Text2Video">https://github.com/sibozhang/Text2Video</a>     （只有测试代码，没有训练部分）</p>
<p><strong>演示视频</strong>：<a target="_blank" rel="noopener" href="https://youtu.be/d5MFzHxeOTs">https://youtu.be/d5MFzHxeOTs</a></p>
<blockquote>
<p>任务</p>
</blockquote>
<p>以任意一段文字作为输入，生成一段表情和口型自然地念出这段文字的talking-head视频</p>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220902151523855.png" alt="image-20220902151523855"></p>
<blockquote>
<p>动机</p>
</blockquote>
<p>本文相较于基于语音的视频生成方法的三个优势：</p>
<ul>
<li>仅仅需要很少的训练视频就可以完成预处理训练</li>
<li>由于采用了TTS，模型不会受到讲话者影响</li>
<li>处理，训练，推断速度大幅提升</li>
</ul>
<blockquote>
<p>方法</p>
</blockquote>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220902151902492.png" alt="image-20220902151902492"></p>
<ul>
<li>
<p>方法总体概述：给定输入文本，通过百度的TTS生成对应的语音，然后通过P2FA对齐器(aligner）将语音强制对齐获得语音的音素时间戳并在音素-姿态（phoneme-pose）字典中查找对应的音素姿态（phoneme poses），再通过关键姿态插值和平滑处理生成姿态序列，最后通过vid2vid GAN生成最终的视频</p>
<ul>
<li>系统的输入的文本可以是任意形式（英文，中文，数字以及标点符号）</li>
<li>时间戳：每个文字在音频中的时间位置</li>
</ul>
</li>
<li>
<p><strong>phoneme-pose字典生成</strong>：</p>
<ul>
<li>英语音素表和汉字声韵母表</li>
</ul>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220902163622330.png" alt="image-20220902163622330"></p>
<ul>
<li>关键姿态抽取：使用Openpose从训练视频中通过平均所有的音素姿态提取关键口型姿态</li>
<li>音素抽取：使用了P2FA对齐器(aligner,来确定发声及其发声的时间位置。该任务需要两个输入：音频和单词转录。预先将转录的单词映射到 phone sequence中。通过比较观察到的语音信号和预训练的基于隐马尔可夫模型（HMM）的声学模型来确定phone boundaries。在强制对准中，语音信号被分析为连续的一组帧（例如，每10ms）。在给定观察数据和HMM表示的声学模型的情况下，通过找到最可能的隐藏状态序列来确定帧与音素的对齐方式。然后，根据对齐方式为字典中的每个音素存储pose sequences。根据数据集的视频帧速率和平均讲话速率来确定phoneme-poses的宽度。</li>
</ul>
</li>
<li>
<p><strong>Key Pose Insertion</strong>:</p>
<ul>
<li>如果两个音素关键姿势帧之间的间隔长度大于或等于最小关键姿势距离，我们将使用Key pose~i~ 和Key pose~i+1~进行插值。</li>
<li>如果两个音素关键姿态帧之间的间隔长度小于最小关键姿态距离，我们将跳过Key pose~i+1~，使用Key pose~i~和Key pose~i+2~进行插值。</li>
<li>最后使用插值将两个关键姿势序列之间的关键姿势与音素姿势的加权和混合，如图 下图所示。输出序列中的新帧被插值在两个关键姿势帧之间，根据它们与那些帧的距离加权两帧。权重与关键帧的距离成反比，即距离越大，权重越小。</li>
</ul>
</li>
</ul>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220902151937267.png" alt="image-20220902151937267"></p>
<ul>
<li><strong>Smoothing</strong>：这里就是为了不影响精度对以嘴部为中心的区域不进行平滑处理</li>
</ul>
<blockquote>
<p>实验</p>
</blockquote>
<ul>
<li>实验数据集相关信息</li>
</ul>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220902161810683.png" alt="image-20220902161810683"></p>
<ul>
<li>同其他audio-driven的sota方法效果比较（上），使用原视频文本生成不同类型声音版本跟原始视频进行比较（下）</li>
</ul>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220902161848953.png" alt="image-20220902161848953"></p>
<ul>
<li>生成过程可视化</li>
</ul>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220902165209440.png" alt="image-20220902165209440"></p>
<ul>
<li>同其他方法训练，处理和推断时间的比较</li>
</ul>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220902165240277.png" alt="image-20220902165240277"></p>
<blockquote>
<p>自己的一些看法</p>
</blockquote>
<ul>
<li>最终作者的演示视频中展示的效果看起来怪怪的，而且评估指标均采用人为的主观评价，目前刚接触这一领域，感觉用这种方式去评估方法的好坏并不是很客观</li>
<li>而且我总觉得一般人说话除了口型外，面部表情大部分是跟语义相关的而并非语音，按照作者的这种方法，将音素同讲话者姿态绑定就会看起来怪怪的</li>
</ul>
<h2 id="AD-NeRF-Audio-Driven-Neural-Radiance-Fields-for-Talking-Head-Synthesis">AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis</h2>
<p><strong>论文连接</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.11078">https://arxiv.org/abs/2103.11078</a></p>
<p><strong>GitHub代码地址</strong>：<a target="_blank" rel="noopener" href="https://github.com/YudongGuo/AD-NeRF">https://github.com/YudongGuo/AD-NeRF</a></p>
<p><strong>演示视频</strong>：<a target="_blank" rel="noopener" href="https://v.youku.com/v_show/id_XNTEyNzY5MTA0NA==.html">https://v.youku.com/v_show/id_XNTEyNzY5MTA0NA==.html</a></p>
<blockquote>
<p>动机</p>
</blockquote>
<blockquote>
<p>方法</p>
</blockquote>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220906155734957.png" alt="image-20220906155734957"></p>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220906155812709.png" alt="image-20220906155812709"></p>
<blockquote>
<p>实验</p>
</blockquote>
<blockquote>
<p>自己的一些看法</p>
</blockquote>
<ul>
<li>实际跑下来，效果比较一般，整个训练过程时间比较长，不知道是哪里出了问题。</li>
</ul>
<h2 id="Audio-driven-Talking-Face-Video-Generation-with-Learning-based-Personalized-Head-Pose">Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose</h2>
<p><strong>论文连接</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.10137v2.pdf">https://arxiv.org/pdf/2002.10137v2.pdf</a></p>
<p><strong>GitHub代码地址</strong>：<a target="_blank" rel="noopener" href="https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose">https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose</a></p>
<p><strong>演示视频</strong>：<a target="_blank" rel="noopener" href="https://cg.cs.tsinghua.edu.cn/people/~Yongjin/video_arxiv.mp4">https://cg.cs.tsinghua.edu.cn/people/~Yongjin/video_arxiv.mp4</a></p>
<h2 id="A-Lip-Sync-Expert-Is-All-You-Need-for-Speech-to-Lip-Generation-In-The-Wild">A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild</h2>
<p><strong>论文连接</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.10010v1.pdf">https://arxiv.org/pdf/2008.10010v1.pdf</a></p>
<p><strong>GitHub代码地址</strong>：<a target="_blank" rel="noopener" href="https://github.com/Rudrabha/Wav2Lip">https://github.com/Rudrabha/Wav2Lip</a></p>
<p><strong>演示视频</strong>：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=0fXaDCZNOJc">https://www.youtube.com/watch?v=0fXaDCZNOJc</a></p>
<blockquote>
<p>任务 （内容简介）</p>
</blockquote>
<p>文章所研究的问题任务如下图的左半部分：根据一段语音和一段talking-head视频生成口型同步视频，根据演示视频可以看到这篇文章最终的效果还是很不错的。</p>
<p>作者在文章开头就给出这项任务的若干非常有意义的实际应用场景</p>
<ul>
<li>电影中配音的口型同步</li>
<li>高质量课程跨语言同步*</li>
<li>重要会议跨语言同步转播*</li>
<li>修复因某种情况缺失（或卡顿）画面的视频记录*</li>
</ul>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220922132817001.png" alt="image-20220922132817001"></p>
<blockquote>
<p>动机</p>
</blockquote>
<p>这篇文章的主要贡献如下：</p>
<ul>
<li>一个可以对任意语音和任意talking-head视频进行口型同步模型</li>
<li>重新定义了一种新的评估标准和指标</li>
<li>收集发布一个用于对任意视频进行口型同步的评估数据集ReSyncED</li>
</ul>
<blockquote>
<p>方法</p>
</blockquote>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220921131235883.png" alt="image-20220921131235883"></p>
<ul>
<li>
<p><strong>Pre-trained Lip-Sync Expert</strong>: 首先是全文重点介绍的<strong>口型同步鉴别器</strong></p>
<ul>
<li>SyncNet：下图是原始SynNet网络模型示意图，本文的口型同步鉴别器就是在这个网络的基础上修改训练得到的
<ul>
<li>彩色图像输入</li>
<li>卷积层中添加残差连接，卷积层更深</li>
<li>使用余弦相似度和二元交叉熵损失</li>
</ul>
</li>
</ul>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220922163346148.png" alt="image-20220922163346148"></p>
<ul>
<li>Sync loss:</li>
</ul>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220922170110682-16638372837191.png" alt="image-20220922170110682"></p>
</li>
<li>
<p><strong>整个模型简要概述</strong>：</p>
<ul>
<li>输入：声音信号、对应的视频片段以及等长的随机视频片段</li>
<li>过程：声音信号通过语音编码器生成语音特征；随机视频片段同mask一半的对应视频进行concat后通过身份编码器生成身份特征；两个特征通过人脸解码器生成结果；结果视频片段和语音送入Pre-trained Lip-Sync Expert得到Sync loss；生成结果同对应视频片段进行比较得到L1重建损失；最后通过Video Quality Discriminator进行鉴别</li>
</ul>
</li>
<li>
<p><strong>模型生成架构的一些细节</strong>：（懒得用语言描述了直接上代码）</p>
<ul>
<li>身份编码器</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">self.face_encoder_blocks = nn.ModuleList([</span><br><span class="line">	nn.Sequential(Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">1</span>, padding=<span class="number">3</span>)), <span class="comment"># 96,96</span></span><br><span class="line"></span><br><span class="line">	nn.Sequential(Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>), <span class="comment"># 48,48</span></span><br><span class="line">	Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">	Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>)),</span><br><span class="line"></span><br><span class="line">	nn.Sequential(Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),    <span class="comment"># 24,24</span></span><br><span class="line">	Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">	Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">	Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>)),</span><br><span class="line"></span><br><span class="line">	nn.Sequential(Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),   <span class="comment"># 12,12</span></span><br><span class="line">	Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">	Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>)),</span><br><span class="line"></span><br><span class="line">	nn.Sequential(Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),       <span class="comment"># 6,6</span></span><br><span class="line">	Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">	Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>)),</span><br><span class="line"></span><br><span class="line">	nn.Sequential(Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),     <span class="comment"># 3,3</span></span><br><span class="line">	Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),),</span><br><span class="line">	</span><br><span class="line">	nn.Sequential(Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),     <span class="comment"># 1, 1</span></span><br><span class="line">	Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)),])</span><br></pre></td></tr></table></figure>
<ul>
<li>语音编码器</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">self.audio_encoder = nn.Sequential(</span><br><span class="line">	Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">	Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">	Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">	Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=(<span class="number">3</span>, <span class="number">1</span>), padding=<span class="number">1</span>),</span><br><span class="line">	Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">	Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">	Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">	Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">	Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">	Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=(<span class="number">3</span>, <span class="number">2</span>), padding=<span class="number">1</span>),</span><br><span class="line">	Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line">	Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">	Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),)</span><br></pre></td></tr></table></figure>
<ul>
<li>人脸解码器</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">self.face_decoder_blocks = nn.ModuleList([</span><br><span class="line">    nn.Sequential(Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),),</span><br><span class="line"></span><br><span class="line">    nn.Sequential(Conv2dTranspose(<span class="number">1024</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>), <span class="comment"># 3,3</span></span><br><span class="line">    Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),),</span><br><span class="line"></span><br><span class="line">    nn.Sequential(Conv2dTranspose(<span class="number">1024</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>),</span><br><span class="line">    Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">    Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),), <span class="comment"># 6, 6</span></span><br><span class="line"></span><br><span class="line">    nn.Sequential(Conv2dTranspose(<span class="number">768</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>),</span><br><span class="line">    Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">    Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),), <span class="comment"># 12, 12</span></span><br><span class="line"></span><br><span class="line">    nn.Sequential(Conv2dTranspose(<span class="number">512</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>),</span><br><span class="line">    Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">    Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),), <span class="comment"># 24, 24</span></span><br><span class="line"></span><br><span class="line">    nn.Sequential(Conv2dTranspose(<span class="number">320</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>), </span><br><span class="line">    Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">    Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),), <span class="comment"># 48, 48</span></span><br><span class="line"></span><br><span class="line">    nn.Sequential(Conv2dTranspose(<span class="number">160</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, output_padding=<span class="number">1</span>),</span><br><span class="line">    Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),</span><br><span class="line">    Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, residual=<span class="literal">True</span>),),]) <span class="comment"># 96,96</span></span><br></pre></td></tr></table></figure>
<ul>
<li>L1重建损失: 生成图像与真值图的像素级的L1损失</li>
</ul>
</li>
<li>
<p><strong>Video Quality Discriminator</strong>(视觉质量鉴别器): 这个在代码中还没看明白</p>
</li>
<li>
<p>评估指标</p>
<ul>
<li>LSE-D，这个指标是通过计算SyncNet中的口型和音频特征之间的距离的平均误差。该指标越低表示视听匹配越好，即语音和嘴唇运动是同步的。</li>
<li>LSE-C，这个指标是平均置信度。该指标越高表示音视频的相关性越好。</li>
</ul>
</li>
<li>
<p>ReSyncED数据集</p>
<ul>
<li>所有的视频音频均来自Youtube，然后主要分成下面三类</li>
<li>Dubbed，音频不同步。主要包括一些配音的电影造成的音频错位以及翻译成其他语言后导致的音频不一致问题的视频</li>
<li>Random，这个随机就是说，随便从某个视频截取一段语音然后同另一个视频（或者这个视频的另一段）组成一个音频（audio-visual）对</li>
<li>TTS，文本转语音，然后和一些原始视频进行组合</li>
</ul>
</li>
</ul>
<blockquote>
<p>实验</p>
</blockquote>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220921131304495.png" alt="image-20220921131304495"></p>
<p><img src="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/image-20220921131333762.png" alt="image-20220921131333762"></p>
<blockquote>
<p>自己的一些看法</p>
</blockquote>
<ul>
<li>直接用作者提供的通过LRS2数据集训练得到的权重对一段视频和随机的语音进行生成，效果非常不错，但仍存在一定的伪影以及部分不说话的情况下嘴唇仍然在动的现象。</li>
<li>后续有文章在此基础上添加了注意力机制进一步提升性能</li>
<li>同TTS进行结合可以变成txt2video模型</li>
</ul>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/text2video/">text2video</a><a class="post-meta__tags" href="/tags/%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90/">视频生成</a><a class="post-meta__tags" href="/tags/%E8%B7%A8%E6%A8%A1%E6%80%81/">跨模态</a></div><div class="post_share"><div class="social-share" data-image="/2022/09/02/text2video%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AE%80%E8%AE%B0/cover.png" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/08/24/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%87%8D%E7%BD%AE%E8%AE%B0%E5%BD%95/"><img class="next-cover" src="/2022/08/24/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%87%8D%E7%BD%AE%E8%AE%B0%E5%BD%95/cover.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">服务器重置记录</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Text2Video-Text-driven-Talking-head-Video-Synthesis-with-Personalized-Phoneme-Pose-Dictionary"><span class="toc-number">1.</span> <span class="toc-text">Text2Video: Text-driven Talking-head Video Synthesis with Personalized Phoneme - Pose Dictionary</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AD-NeRF-Audio-Driven-Neural-Radiance-Fields-for-Talking-Head-Synthesis"><span class="toc-number">2.</span> <span class="toc-text">AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Audio-driven-Talking-Face-Video-Generation-with-Learning-based-Personalized-Head-Pose"><span class="toc-number">3.</span> <span class="toc-text">Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-Lip-Sync-Expert-Is-All-You-Need-for-Speech-to-Lip-Generation-In-The-Wild"><span class="toc-number">4.</span> <span class="toc-text">A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By ss967</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">陕ICP备2020017270号</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>